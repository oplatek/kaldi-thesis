% !TEX root = main.tex
\chapter{Background}
\label{cha:background}

The statistical methods for continuous speech recognition were established more than 30 years ago. 
The most popular statistical methods are based on \ac{HMM} \acl{AM} and n-grams \ac{LM},
which are also used in Kaldi, the toolkit of our choice.

Section~\ref{sec:back_asr} introduces speech preprocessing, \ac{AM} and \ac{LM} training
and explains the principle of speech decoding.

The~three sections describe decoding or training models for a~particular software.
The Kaldi toolkit is described in Section~\ref{sec:back_kaldi}, 
the \ac{HTK} toolkit in Section~\ref{sec:back_htk} and 
the Julius decoder in Section~\ref{sec:back_julius}.

\begin{figure}[!htp]
    \begin{center}
    \input{images/asr-components}
    \caption{Architecture of statistical speech recognizer\cite{ney1990acoustic}}
    \label{fig:components} 
    \end{center}
\end{figure}

\section{Automatic speech recognition}
\label{sec:back_asr}

The goal of statistical \ac{ASR} is to decode 
the best speech transcription for given speech recording.
Formally, we search for the most probable sequence of~words $w^*$ given the acoustic observations $a$.
See Equation~\ref{eq:best_seq}, which can be simplified to Equation~\ref{eq:best_fix},
because we want decode $w^*$ for fixed speech $a$.

\begin{equation}\label{eq:best_seq}
    w^* = argmax_{w}\{P(w \mid a)\} = argmax_{w}\{\frac{P(a \mid w) * P(w)}{P(a)}\}
\end{equation}

\begin{equation}\label{eq:best_fix}
    w^* = argmax_{w}\{P(w \mid a)\} = argmax_{w}\{P(a \mid w) * P(w)\}
\end{equation}

The task of acoustic modeling is to estimate the probability $P(a \mid w)$,
which we describe in Section~\ref{sub:am}. 
Similarly, the \ac{LM} represents the probability $P(w)$.
We describe language modeling in Section~\ref{sub:lm}.

Improving the accuracy of speech recognition engine is dependent
mainly on improving the \ac{AM} and also the \ac{LM}.
The \ac{AM} and \ac{LM} are trained separately 
and stored on a~hard disc.

Typically it is easier to model preprocessed speech, 
so various feature and model space transforms are applied before \ac{AM} training.
The speech recogniser has to use the same speech preprocessing,
which is used for \ac{AM} model training.
See Figure~\ref{fig:components}.


% {\it Supervised learning}\/ is the~machine learning task of inferring a function from labeled training data.\cite{mohri2012foundations}. 
%Labeled data is set of training examples pairs. The pairs consists of input features and desired output as captured in~Figure~\ref{fig:supervised}. The supervised learning algorithm infers a function partially influenced by the training examples and partially determined by the algorithm itself. In ideal case, the inferred function would be able to predict correct output values for unseen input data. 
% The inferred function is often called {\it model}. 
% 
% {\it Unsupervised learning}\/ can be thought of as finding patterns in the input data beyond what would be considered 
% pure unstructured noise\cite{ghahramani2004unsupervised}. Very often unsupervised learning is an application 
% of~statistical methods as in language modeling depicted in~Figure~\ref{fig:unsupervised}


\subsection{Speech parameterisation}
\label{sub:param}
% \section{Signal processing}
% FIXME consult kaldi website with what I have written
% http://kaldi.sourceforge.net/feat.html
The goal of speech parameterization is to reduce the negative environmental influences on speech recognition.
The speech varies in a~number of aspects. Some of them are listed below:
\begin{itemize}
    \item Differences among speakers pronunciation depends on gender, dialect, voice, etc.
    \item Environmental noises. In the~application for a dialog system the typical speech is
        recorded on a~noisy street. 
    \item The recorded channel. 
        For example the telephone signal is reduced to frequency band between 300 to 3000Hz.
        The quality of mobile phone signal also influences the quality of the audio signal.
\end{itemize}

Speech parametrisation improves robustness of the speech recognition in different recording conditions.
Note that parametrisation of speech transforms the acoustic signal based on few numerical parameters,
which are constant for all recording conditions during training and decoding.
Simply put, the acoustic signal is transformed by boosting frequencies, which are informative for speech.

\ml{acoustic features}
Speech parametrisation extracts speech-distinctive acoustic features from raw waveform.
The two most successful methods for speech parametrisation in last decades are 
\ac{MFCC}\cite{davis1980comparison} and \ac{PLP}\cite{hermansky1990perceptual}.
% change ti to something else than 
% SPEECH SIGNAL PARAMETERIZATION TECHNIQUES FOR SPEECHUNDERSTANDING SYSTEM Gaurav K. LEEKHA, Mrs. Meenakshi MEHLA, Shrilekha SAGWAL
The methods are very computationally effective and significantly improve the quality of recognised speech.


The toolkits used in our dialog system, Kaldi and \ac{HTK} toolkit,
compute \ac{MFCC} coefficients for given audio input similarly.\footnote{The~subtle differences are caused by implementation approaches, but does not effect the quality of~\ac{MFCC} coefficients in~significant way.}
We choose \ac{MFCC} as speech parametrisation technique for both toolkits,
so we can compare them.

% An interested reader can find comprehensive introduction into signal processing 
% in~the~fourth Chapter of Spoken Language Processing book\cite{huang2001spoken}.
Both \ac{MFCC} and \ac{PLP} transformations are applied on a~sampled and quantized audio signal.
The sampling frequency is typically 8000, 16000 or 32000 Hz.
Each sample is usually encoded to 8, 16 or 32 bits. 
In our experiments we use 16 kHz sampling frequency for 16 bit samples.  

\ml{feature window}
The \ac{MFCC} or \ac{PLP} statistics are computed for signal for overlapping windows.
In Figure~\ref{fig:mfcc_window} there are 7 windows 
for audio of length 85 ms and window shift and length 10ms and 25 ms.

\begin{figure}[!htp]
    \begin{center}
    \input{images/mfcc_window}
    \caption{\ac{PLP} or \ac{MFCC} features are computed every 10 ms seconds in 25 ms windows.
    Audio length is $(frames-1)*shift + win\_len = 85ms$}
    \label{fig:mfcc_window} 
    \end{center}
\end{figure}

For each window \ac{MFCC} or \ac{PLP} efficiently computes statistics with a~reduced dimension. 
Let us describe the~\ac{MFCC} computation for 25 ms window shifted by 10 ms and 16kHz audio sampling frequency. 
The $16000 * 0.025 = 400$ samples in one window are reduced to 13 static cepstral coefficients.

The \ac{MFCC} or \ac{PLP} static features are usually extended 
by time derivatives delta and delta-delta features \cite{psutka2001comparison},
so it results to $13 + 13 + 13 = 39$ \ac{MFCC} delta \& delta-delta features 
per 400 samples in one window.

The \ac{MFCC} features are computed by the~following steps:

\small{\begin{enumerate}
    \item The~audio samples are transformed into {\it frequency domain}\/ by~\ac{DFT} in the window.
    \item The frequency spectrum from the~previous step is transformed onto the mel scale, 
        using triangular overlapping filters.
    \item From the mel frequencies the logs of the powers are taken from each of the mel frequencies.
    \item At the end the discrete cosine transform is applied on the list of mel log powers.
    \item The \ac{MFCC} coefficients are the amplitudes of the resulting spectrum.
    \item The delta and delta-delta coefficients are computed from the current and previous static features.
\end{enumerate}

\subsection*{Feature space transformations}
Feature space transformations are usually applied in addition to \ac{MFCC} or \ac{PLP} preprocessing.
\ml{frame}
The feature space transformations are applied per frame. 
Each feature vector extracted from \ac{MFCC}
or \ac{PLP} window - frame, is projected to another space.
Some transformations take into account context of several 
preceding (left context) and consecutive frames (right context).

The linear or affine transforms are expressed by matrix multiplications $Ax$ respectively $Ax^+$.
The matrix $A$ represents the transformation. 
The $x$ is the input vector and $Ax$ are the transformed features.
The affine transformations uses extended vector $(x^+)^T = (x_1, \ldots, x_n, 1)$ and matrix $A: (n+1)*(n+1)$.

There is a large variety of available transforms and 
dependent on your acoustic data you should choose the most appropriate one.
The transforms differ according to how they are estimated,
namely for linear transforms how the matrix $A$ is trained.

Some transforms are trained on all acoustic data, some are per speaker.
Some transforms are estimated discriminatively, some use generative models.

We list some of Kaldi transforms in order to illustrate
rich choice of feature transforms in Kaldi toolkit.
In Section~\ref{sec:transform} we describe in more detail the transforms,
which we use in our training scripts.

% use and city http://kaldi.sourceforge.net/transform.html#transform_cmllr_global
\begin{itemize}
    \item \acl{LDA}\cite{gopinath1998maximum}
    \item \acl{HLDA}\cite{gales1999semi}
    \item \acl{MLLT} also known as \acl{STC}\cite{gopinath1998maximum}
    \item \acl{ET}Exponential Transform (ET)\cite{povey2011exponential}
    \item \acl{CMVN}\cite{molau2003feature}
\end{itemize}


% To conclude, the signal processing samples the continuous audio at regular interval intervals. 
% Fixed number of consecutive samples forms a window. The~windows are usually overlapping.
% The~acoustic features are computed for each of the overlapping window. 

% section signal_processing (end)

\subsection{Acoustic modeling}
\label{sub:am}
Acoustic modeling is arguably the heart of the~speech recognition.
More realistic modeling of acoustic features directly affects the speech recognition quality 
as seen in Equation~\ref{eq:best_fix}. 
The \ac{AM} estimates the probability $P(a|w)$ of generating acoustic features $a$
for a~given word $w$.

The acoustic modeling has only partial information available for training \ac{AM}.
Let us mention that acoustic features $a$ are collected every 10 ms.
However, the corresponding textual transcription is time-unaligned.
The hidden information of words time alignment in a sentence
makes acoustic model training more challenging.

Modern speech recognition toolkits use \acl{HMM}
for modeling uncertainty between acoustic features and the~corresponding transcription. 

\subsubsection*{Choice of training units}
The most successful acoustic modeling methods do not estimate the $P(a|w)$ directly,
but estimate probability $P(a|f_{b}f_{c}f_{c})$ of generating acoustic features $a$ for triphone $f_{b}f_{c}f_{c}$.

\ml{phoneme}
Phone is the smallest contrastive unit of speech. 
Let us see few examples of words and their phonetic transcriptions according CMU dictionary\cite{weide1998cmu}.
\begin{itemize}
    \item {\it youngest} \& {\it  Y AH1 NG G AH0 S T }
    \item {\it youngman} \& {\it  Y AH1 NG M AE2 N }
    \item {\it earned} \& {\it ER1 N D}
    \item {\it ear}\/ with two transcribed pronunciations {\it IY1 R}\/ and {\it IH1 R}
\end{itemize}
The CMU dictionary distinguishes among several variations for each vowel e.g. {\it AH1}\/ and {\it AH0}.
It distinguishes the~pronunciation for {\it earned}\/ and {\it ear}\/
despite the same transcription in the~first three letters.
It also stores two possible pronunciations for the word {\it ear}.

The acoustic features for a phone significantly depend on the~context.
The previous and the~following phone strongly influence the sound of the phone in the middle.

\ml{triphone}
The triphone is a~sequence of three phonemes and captures the context of single phone.
See~\ref{fig:hmm_words}.
The triphones variate much less according to the~context than phonemes, so they can be easily trained.
Let us note that certain combinations of prefixes have the same effect on the central phoneme,
e.g. {\it q}\/ and {\it k}\/ has the same effect on {\it i}. % cite??
In order to reduce the number of triphones, these triphones are clustered together.

The advantage of training triphone-\ac{AM} over word-\ac{AM} 
is due to words sparsity. 
There is typically less clustered triphones and many words for training data,
so a triphone model can be trained from much more examples than a word model. 

% TODO mentioned curse of dimensionality for words -> more parameters than triphones for less examples

\begin{figure}[!htp]
    \begin{center}
    \input{images/hmm-words}
    \caption{Markov Triphone model for three words. Transition probabilities within
    words and observation probabilities are learnt.}
    \label{fig:hmm_words} 
    \end{center}
\end{figure}


% \begin{figure}[!htp]
%     \begin{center}
%     \input{images/supervised-general}
%     \input{images/supervised-baum-welsh}
%     \caption{Supervised learning idea and example}
%     \label{fig:supervised} 
%     \end{center}
% \end{figure}

\subsubsection*{Hidden Markov Models}
The~\ac{HMM} is a~very powerful statistical method for~characterizing observed data samples
of~a~discrete-time series with an unknown state. Chapter 8, \cite{huang2001spoken}.
In our case the hidden states represent triphones and we observe samples of the~acoustic features.

\ml{transition probability}
Hidden Markov Models have two type of parameters {\it transition probabilities among states}\/
and {\it probabilistic distribution for generating observation in given state}.
The transition probability is a~probability of changing state $q$ to state $u$.
Each transition can be represented as arc $e=qu$ between the states $q$ and $u$, see~\ref{fig:hmm_words}.
The probability is often represented as the weight $w_e$ of arc $e$.
For every node in a Markov model must hold that the sum weights of outgoing arcs is one.

The Markov model emits deterministically in each node an observation during traversal over its arcs.
The \acl{HMM} emits the observation stochastically based on the probabilistic distribution related
to~the~visited state.

\ml{Gaussian HMM}
For speech recognition the multivariate Gaussian distribution is used in all states of \ac{HMM}. 
Typically the Gaussian distribution in each state models acoustic features $a$.
Ideally the estimated \ac{HMM} \ac{AM} would generate in each state features belonging to one triphone.

\subsubsection*{Training \ac{HMM}}
\label{sub:trainhmm}

The Kaldi as well as the~\ac{HTK} toolkit uses \acl{EM} algorithms to train \ac{HMM} \acl{AM}.
The toolkits models the observation probabilities using multivariate Gaussian distribution 
with dimension of the acoustic features $a$.
The \ac{EM} algorithm starts with \ac{HMM} with initial values. 

Typically, the~transition probabilities follow uniform distribution.
The observation probabilities are usually initialized by multivariate Gaussian distribution
with $\mu$ and $\Sigma$ set to global mean and global covariance matrix 
estimated on all training acoustic data.

\ml{EM}
Let us describe how the \ac{EM} algorithm operates for one pair
of training data consisting of acoustic features $a$ extracted from speech
and corresponding text speech transcription $t$.
We create \ac{HMM} $t'$, where each state represents one triphone. 
The triphones are extracted from transcription $t$ using pronunciation dictionary.
In Figure~\ref{fig:hmm} the transcription {\it how do you do}\/ 
was the~expanded \ac{HMM} model representing triphone sequence.
It should be obvious that only parameters of HMM states 
occurring in $t'$ can be updated from pair $(a, t)$.

The \ac{EM} algorithm iterates following steps in order 
to update parameters of transition and observation probabilities:
\begin{itemize}
    \item The observation probabilities are computed using \ac{HMM} $t'$. 
    \item {\bf E-step}: Based on the observation probabilities the observation are assigned to states of \ac{HMM} $t'$. 
    \item {\bf M-step}: Based on assignment of observation to states the $t'$ parameters are re-estimated. 
\end{itemize}

\ml{Baum-Welsch}
The acoustic model training conceptually differs for \ac{HTK} and Kaldi only in the {\bf expectation (E) step}.
The \ac{HTK} toolkit in {\bf E-step} finds the most probable distribution 
which map acoustic observation to \ac{HMM} states using \ac{MLE}. \todo{MLE fill few words and cite}. 
The Baum-Welsch equations can be derived from the fact, 
that the~\ac{MLE} criterion is used for finding the most probable distribution in {\bf M-step}.\cite{huang2001spoken}
This variation of \ac{EM} algorithm for estimating \ac{HMM} parameters is called Baum-Welsch algorithm.

\ml{Viterbi}
On the other, hand the Kaldi toolkit applies the Viterbi 
criterion in assigning the acoustic observation to \ac{HMM} states.
Using the Viterbi criterion Kaldi just finds 
the most probable assignment $\phi$ of observations to states.\cite{buthpitiya2012parallel}
Using only the single best assignment $\phi$ the {\bf M-step} is also simpler than
Baum-Welch algorithm case.
We simply refer to the~\ac{EM} algorithm with Viterbi criterion as Viterbi training.

\subsubsection*{Viterbi training of acoustic models}
We focus on Viterbi training because it is used in Kaldi toolkit and it is simpler.
Latest work suggest that Viterbi training is just as effective for continuous
speech recognition as Baum-Welch algorithm \cite{rodriguez2003comparative}.
In addition Viterbi training needs much less computational resources. 

\todo{Add equations from buthpitiya2012parallel!}

\subsection{Language modeling}
\label{sub:lm}

\ac{LM} effectively reduces and more importantly priorities the \ac{AM} hypothesis.
The statistical \ac{LM} assigns a given word sequence its probability.
The probability of a word transcription from \ac{AM} is combined with
the probability of the word transcription from \ac{LM}.

The invalid words sequences or the sequences not frequent in training data
are estimated with a low probability. On the other hand the frequent sequences
are assigned with high probability.

The word sequence "{\it are few born new york}" should be rewarded 
with a lower probability from \ac{LM} than the alternative with similar phonetic representation
"{\it are you from new york}".

Note that in our settings the \ac{AM} already limits the possible triphone
sequences to sequences present in lexicon words - the words in training data.
% The lexicon bridges the \ac{AM} and \ac{LM}
So the mapping from acoustic features $a$ to triphone sequences
are restricted to sequences, which form probably word sequences.

In practice it is unfeasible to compute the probability
of $k$ word sequence $W$ according Equation~\ref{eq:lm}.
In addition, it would require enormous amount of data for estimating
probability for $k>10$.


\begin{equation} \label{eq:lm}
    P(W)=P(w_i,  w_{i-1}, w_{i-2}, ..., w_1)=\prod_{i=1}^{k}{P(w_i|w_{i-1}, w_{i-2}, w_1)}
\end{equation}

\ml{\ac{LM} order}
We call the number $k$ order of \ac{LM}.
In speech recognition we typically use the order of \ac{LM} limited to $k<4$.
The Equation~\ref{eq:ngram} describes how the trigram \ac{LM} should be estimated.
The n-gram is a sequence of n consecutive words. The probability of n-gram can be estimated by language models
of order n and greater.

The \ac{LM} estimates the probability of an n-gram $t$ by counting the frequency in text corpus of $t$ and 
also (n-1), (n-2), .. n-grams, which are subsequences of $t$.
The text corpus is typically chosen, so it corresponds to a domain, in which the \ac{ASR} system 
with the \ac{LM} should be used.

\ml{LM smoothing}
We call the {\it smoothing}\/ technique estimating the probability of unseen n-grams $t$ 
based on n-grams with lower order which form the original sequence $t$.
The technique is widely used, because there is much greater chance that lower order n-grams are
present in training data than n-gram with high order.
For example, in our training scripts if no external \ac{LM} is supplied,
we train the \ac{LM} only on text transcriptions from the training data
and we use Witten-Bell smoothing.\cite{witten1991zero}

\begin{equation} \label{eq:ngram}
    P(w_i \mid w_{i-1}, w_{i-2}, ..., w_1) = P(w_i \mid w_{i-1}, w_{i-2}) * P(w_{i-1} \mid w_{i-2}) * P(w_{i-2})
\end{equation}

\subsection{Speech decoding}
\label{sub:decode}
The \ac{CSR} is a pattern recognition task as well as a search problem.
In speech recognition, making a search decision is also referred to as decoding.\cite{huang2001spoken}


For isolated word recognition the \acp{HMM} are trained for each word.
Using the forward algorithm for each \ac{HMM} $h$ representing a word $w$, 
we are able to compute the probability  of every $w$ given the acoustic observations $a$.
The isolated word recognition becomes a~simple recognition problem,
where we select the most probable \ac{HMM} $h^*$ from a finite set of \acp{HMM},
which parameters were trained either by \ac{EM} algorithm as described in Subsection~\ref{sub:trainhmm}.

Note that the \ac{HMM} training is identical for \ac{CSR} and isolated word recognition,
but the decoding is more complicated for \ac{CSR}.

\begin{figure}[!htp]
    \begin{center}
        \input{images/hmm-lm}
        \caption{Diagram of how \ac{LM} is combined with \acp{HMM}.}
        \label{fig:hmm_lm} 
    \end{center}
\end{figure}

Imagine we have a \ac{LM} order 1 of only two words - {\it one}\/ and {\it two}\/ each uniformly distributed\footnote{
If a \ac{LM} of order 1 assigns to every word equal probability, we say it has order 0}.
We want to decode any possible sequence of words {\it one}, {\it two}, so 
we add additional $\epsilon$ transition at the end of words \acp{HMM}
and connect them so that we can chain the words as illustrated in Figure~\ref{fig:hmm_lm}.
Figure~\ref{fig:hmm_alt} shows expanded \acp{HMM}, which have $\epsilon$ transitions
at the beginning and the end. 

\todo{Note that \ac{LM} weight can be stored on the in the $\epsilon$ transition at the beginning.}

\todo{Figure~\ref{fig:hmm_alt} add $P(w)$ on the arc from the start state. Denote the states S and F}
\begin{figure}[!htp]
    \begin{center}
        \input{images/hmm-alt}
        \caption{Expanded \acp{HMM} for words {\it one}\/ and {\it two}\/. 
        The arrows at \ac{HMM} states illustrate that every observation of acoustic features 
        can be generated according to the~statistical distribution. 
        Note that if a speaker says {\it two} a \ac{HMM} model with well trained parameters should output higher probability
        for \ac{HMM} representing {\it two}. For longer speaker sequences e.g. {\it two one one two ...} the \acp{HMM} are connected over the 
$\epsilon$ transitions, and a search is used for selecting the most probable sequence.}
        \label{fig:hmm_alt} 
    \end{center}
\end{figure}

Even the simple \ac{HMM} network in Figure~\ref{fig:hmm_alt} can become
large search problem partially because the search space of words grows
exponentially in number of words in utterance and partially
because the word boundaries are unknown. 
Each word can begin in any moment and last with decreasing probability ad infinity,
so the search space explode.
In our trivial case we skip problems with higher order \acp{LM}, silence modeling
and large lexicon.

As you can see the search space of speech recognition problem with 
large vocabulary is enormous and has to solved in very short time.
The most famous \ac{AI} search algorithm $A^*$ for large search problem can not be used,
because during decoding as user speaks we know little about what is he going to say in~the 
rest of the utterance, so we can not design a good heuristic function.
With $A*$ algorithm we could not exploit the time when user speak
to perform any computation. A user expects a response around 200 ms after his speech, 
so we need the find the best path in very short time.

Natural choice for one-best hypothesis is Viterbi beam search.\cite{huang2001spoken}
We already described Viterbi algorithm in Subsection~\ref{sub:trainhmm}.
The beam is used to limit search space in each iteration of Viterbi algorithm.
The Viterbi algorithm is breadth-first search algorithm and the beam limits 
number of nodes, which are expanded from current set of nodes to next iteration.

\todo{picture beam search}

We list few alternatives how to set up the beam for speech decoding.
\begin{itemize}
    \item {\it Fixed beam} guarantees memory footprint and fast decoding.
    \item {\it One-best hypothesis comparison} effectively discards 
        most of the improbably hypothesis if the one-best hypothesis is significantly better than alternatives
        and keeps lot of alternatives if one-best hypothesis is weak.
        The relative one-best hypothesis comparison naturally broaden the beam in uncertain region,
        but does not guarantee no hard limits.
    \item {\it Combination} of methods applies the strictest criteria on beam in each iteration.
\end{itemize}


So far we focused on combination of \ac{HMM} \ac{AM} and \ac{LM} and its graph representation,
but also mixing the probabilities of \ac{AM} and \ac{LM} according Equation~\ref{eq:best_fix}
is not straightforward in practice. Due to numerous assumption violation for computing 
\ac{AM} as well as \ac{LM} it proves to efficient to use weighted product of the two models.
Typically, language model weight is used $w_{lm}$.

\begin{equation}\label{eq:am_lm}
    w^* = argmax_{w}\{P(w \mid a)\} = argmax_{w}\{P(a \mid w) * P(w)^{w_{lm}}\}
\end{equation}

As you can imagine the path of phones in the search graph 
become rather large easily and even with a beam search lot of hypothesis
are assign with tiny probabilities.
In order to keep the numeric stability, the probabilities 
are computed in log scale. We are searching for the most probable path in a graph,
however the typical search task for is shortest path problem.
In order to use the shortest distance measure to find the most probably path
and avoid multiplication we use formula~\ref{eq:am_lm_log} derived from equation~\ref{eq:am_lm}.
The $C(a \mid w)$ and $C(w)$ are costs with range between zero and one,
where cost of one corresponds to zero probability $C(1) \cong P(0)$ 
and cost of infinity corresponds to one probability $C(\infty) \cong P(1)$.

\begin{equation}\label{eq:am_lm}
    \begin{split}
        w^* &= argmin_{w}\{log(\frac{1}{C(a \mid w) * C(w)^{w_{lm}}})\} \\
        &= argmin_{w}\{-log(C(a \mid w) * C(w)^{w_{lm}})\} \\
        &= argmin_{w}\{-log(C(a \mid w)) - w_{lm}*log( C(w))\} 
    \end{split}
\end{equation}

\subsubsection*{Decoding formats}
In many applications formats, which can represent also alternative hypothesis,
provide better results for further processing than one-best hypothesis,
because the alternatives contain missing information.

\ml{n-best list}
We present n-best list and lattice formats, which both can also represent 
alternative hypothesis.

N-best list is an extension to the one-best hypothesis. 
In n-best list is included apart from the most probable word sequence, also 
the~second, third, \ldots, n-th most probable hypothesis. 
Usually, also the likelihood is reported with each word sequence.
For some applications the likelihood is sufficient,
other application for example in a dialog system prefer
posterior probabilities reported in the n-best list.
Note that the posterior probabilities of 3-best list on Figure~\ref{fig:nbest}
do not sum to one, because n-best list typically omits some hypothesis.

\begin{figure}[!htp]
    \begin{center}
\begin{verbatim}
    0.5 hi how are you
    0.2 hi where are you
    0.1 bey how are you
\end{verbatim}
    \caption{Toy example of 3-best list output with posterior probability for each path.}
    \label{fig:nbest} 
    \end{center}
\end{figure}

The n-best list and lattice are extracted from the beam history.
Note that the Kaldi toolkit always generates state level lattices
\footnote{State level lattices uses states very similar to triphone states}.\cite{povey2012generating} 
The word lattice and later word n-best list can be extracted 
from the state level lattice. 
The procedure is later described in detail in~Chapter~\ref{cha:decoder}.

\begin{figure}[!htp]
    \begin{center}
    \input{images/non_symetric}
    \caption{Small lattice. \todo{numbers to words}}
    \label{fig:toy_lat} 
    \end{center}
\end{figure}


\subsection{Evaluating \ac{ASR} quality}
\label{sub:eval}
\ml{WER}
The quality of speech recognizer is typically measured using \acl{WER}.
The \ac{WER} measure is computed on pairs $(decoded(a),t)$ for acoustic recoding $a$ and their human transcription $t$.
The expression $decoded(a)$  in this section denotes \ac{ASR} hypothesis in one-best format 
as described in Subsection~\ref{sub:decode}.

The pair $(a,t)$ is not supposed to be used in \ac{AM} training,
because we are testing the ability to decode unknown speech.
We should also measure the \ac{ASR} quality on speech from a speaker,
who did not provide speech used for \ac{AM} training, 
because we usually want to decode speech of an unheard speaker.

The \ac{WER} is computed as a minimum edit distance on words between 
pair $decoded(a), t$ typically using edit operations {\it substitution, deletion, insertion}\/ as described in~\ref{eq:edit_dist}.
The effective implementation for computing WER uses dynamic programing and is not computationally intensive,
because \ac{ASR} hypotheses are typically quite short.
\begin{equation} \label{eq:edit_dist}
    WER = 100* \frac{min\_dist(decoded_{AM, LM}(a), t, edit\_operation=\{Subs, Del, Ins\})}{\#\ words\ in\ t}
\end{equation}
\ml{reference}
Note that \ac{WER} is an error function so the ideal value is zero, because for $WER=0$ the hypothesis $decoded(a)$ and 
the reference transcription $t$ are identical. The \ac{WER} 100 for $decoded(a)$ and reference $t$ 
with equal number of words show that every single word is different between $decoded(a)$ and reference $t$.
Despite the fact that \ac{WER} resembles percentage format, it can be bigger than 100. See Figure~\ref{fig:wer400}.
\begin{figure}[!htp]
    \begin{center}
    \begin{verbatim}
        decoded(a) = 'hi hi hi hi'
        t='hello'
        WER = 100 * ( 4 / 1) = 400
    \end{verbatim}
    \caption{The \acs{WER} measure can be greater than 100.}
    \label{fig:wer400} 
    \end{center}
\end{figure}

\subsubsection*{Alternative measures}
\ml{SER}
The \acl{SER} measures how many decoded utterances $decoded(a)$ match exactly its reference $t$
for all pairs $(a, t)$ in test set $T$.
\begin{equation}
    SER = \frac{\sum_{\{(a, t) \in T; decoded(a) = t\}}{1}}{|T|}
\end{equation}

If we are using an alternative output format to one best hypothesis as n-best list or lattice,
we are extracting one-one best hypothesis format for measuring \ac{WER} or \ac{SER}.
On the other hand we are using n-best lists or lattice,
because the one-best hypothesis might be wrong and the alternative
hypothesis may be closer to the~reference.

We do not evaluate any other measurement except for \ac{WER} and \ac{SER},
but note that richness and correctness of alternatives are desired qualities.
The alternatives may contain additional information and are for example used
in a dialog system \acl{SLU} unit which parses \ac{ASR} output.

\subsubsection*{Measuring speed}
\label{sub:the_metrics_in_speech_recognition}
In this thesis we are especially concerned about the~speed of speech decoding,
because the implemented decoder is used in a \acl{SDS}.

\ml{\acl{RTF}}
A very natural measure of the~speed for speech decoding is \acl{RTF},
which expresses how much the recognizer decodes faster than the user speaks.
We measure the \ac{RTF} for each recording as described in~Equation~\ref{eq:rtf}.
\begin{equation}\label{eq:rtf}
    RTF = \frac{time(decode(a))}{length(a)}
\end{equation}
For real time decoding in a dialog system we need $RTF < 1.0$ for all tested utterances $a$.
In other words, the decoding of each utterance $a$ should take less time than
playing the utterance in a music player.
With $RTF < 1.0$ we can decode faster than the user speaks, 
so the hypothesis is ready immediately after the user finishes the speech. 

Note that in our decoder we have two phases of decoding.
The forward decoding is performed as user speaks, but
the backward decoding is triggered at the very end of the speech
as described in~Subsection~\ref{sub:decode}.
The users have to wait at least for~the time when backward decoding is performed,
so we measure \ac{FWRTF} for forward decoding.

\ml{latency}
In real time \ac{SDS} the critical measure is a delay how long the user
has to wait for its answer.
The latency measures the time between the end of the user speech and
the time when a decoder returns the hypothesis, 
which is the most important speed measure for \ac{ASR} component in \ac{SDS}.
Note if $FWRTF < 1.0 $ in our decoder then the latency is the time of backward decoding.


\section{\ac{HTK}}
\label{sec:back_htk}
The \ac{HTK} toolkit is a~set of command line tools, sample scripts and library
for training and decoding \ac{HMM} focused on speech recognition.
\ac{HTK} uses Baum-Welch algorithm for \ac{HMM} parameters estimation.
With the toolkit are distributed two decoders {\it HVite} and {\it HDecode},
which are not designed for real time applications.
{\it HVite} can be used only with unigram or bigram \ac{LM}. 
The {\it HDecode} decoder handles also a trigram \ac{LM}.

The functionality of the library is wrapped by command line programs.
The programs are typically combined in training scripts to train acoustic and language models.
In Figure~\ref{fig:htk_tools} the acoustic models are labelled as "HMMs" 
and the language models in \ac{HTK} are represented in "Networks".
The trained models are used in one of \ac{HTK} decoders e.g. {\it HVite}\/ for decoding
transcriptions, which can be evaluated using {\it HResults}.

\begin{figure}[!htp]
    \begin{center}
    % \input{images/htk_tools}
    TODO fix PSTRICKs error
    \caption{Figure 2.2 from HTK Book 3.4\cite{young2006htk}}
    \label{fig:htk_tools} 
    \end{center}
\end{figure}

The \ac{HTK} use Baum-Welch algorithm to train acoustic models.
The {\it HVite} decoder uses token passing algorithm and Viterbi criterion.
\cite{HTKBook3.4} % chapter 2
Only bigram \ac{LM} can be used with {\it HVite}.
The \{\it HDecode} decoder can handle bigram or trigram language models. 


Let us stress that we use high quality Bash and Perl scripts for training \ac{HTK} \ac{AM}
from Vertanen improved by MatÄ›j Korvas.\cite{vertanen_baseline_2006}\cite{korvas_2014}

The \ac{HTK} toolkit is licensed under a special license\footnote{\url{http://htk.eng.cam.ac.uk/docs/license.shtml}}.
The {\it HDecode} has very similar license condition but can be only used for research purposes.\footnote{You
need to register even to see the license: \url{http://htk.eng.cam.ac.uk/prot-docs/hdecode_register.shtml}}

\section{Julius decoding engine}
\label{sec:back_julius}

% http://julius.sourceforge.jp/juliusbook/en/desc_overview.html
Julius is a~large vocabulary continuous speech engine, which can use \ac{AM} in \ac{HTK} format for decoding.\cite{lee2009julius}
Julius is BSD licensed\footnote{\url{http://www.linfo.org/bsdlicense.html}} and performs almost real-time decoding.

Julius is a~two pass decoder. In the~first pass the forward decoding is performed as
a technique of time synchronous beam search.
The second pass reranks and further prunes the extracted hypothesis from thethe~~pass one.
The second pass is also known as backward decoding.
Bigram \ac{LM} is used for forward decoding and more complex trigram \ac{LM} is used for backward decoding.

Before the implementation of this thesis was finished 
the team of Alex had been interested in Julius, because its ability of real-time decoding 
and confusion 
network\footnote{Confusion network is approximation of lattice described in~Subsection~\ref{sub:lattice}} 
output format.

The Alex team abandoned the Julius decoder for software issues with crashes of the decoder. 
The crashes appeared during backward decoding and extracting
the confusion network from Julius. 
In addition, the crashes were hard to detect,
because Julius used to run in a~separate process and 
the~dialog system Alex were waiting for the lattice output of dead process,
which should had been passed through sockets.

\section{Kaldi}
\label{sec:back_kaldi}

Kaldi is a~speech recognition toolkit consisting of a~library, command line programs, scripts for focus on acoustic modeling.
Kaldi deploys several decoders for evaluation Kaldi \ac{AM}.
Kaldi uses Viterbi training for estimating \ac{AM}. 
Only in special cases of speaker adaptive discriminative training is also used extended Baum-Welsch algorithm\cite{povey2011kaldi}.

The architecture of the~Kaldi toolkit could be separated to Kaldi library and training scripts.
The scripts access the functionality of Kaldi library through command line programs.
The C++ Kaldi library is based on the {\it OpenFST}\cite{allauzen2007openfst} library and 
it uses optimized libraries for linear algebra such as BLAS and LAPACK.
Related functionality is usually grouped in one namespace in C++ code, which usually also corresponds
to one directory on file system. The examples of the namespaces or directories can be seen in Figure~\ref{fig:kaldi_arch}

\begin{figure}[!htp]
    \begin{center}
        \includegraphics[width=25em]{images/kaldi-lib}
        \caption{Kaldi toolkit architecture\cite{povey2011kaldi}}
        \label{fig:kaldi_arch} 
    \end{center}
\end{figure}

\todo{Create fig:TODO\_kaldi\_prog}
The Figure~\ref{fig:TODO_kaldi_prog} list programs used 
for speech parametrisation, feature transforms, acoustic model
training, decoding and other utilities. Kaldi provides very useful standardized scripts which wrap
Kaldi programs or add a~useful functionality. The scripts are located in {\it utils} and {\it steps}
directories and are used in many training script recipes for different corpus data.
In this thesis we created a~new training recipe using the~Kaldi infrastructure and
Czech and English training corpus \cite{korvas_2014}.
The recipe, the data and acoustic modeling scripts are described in~Chapter~\ref{cha:train}.

\subsection{Finite State Transducers} 
\label{sec:fst}
The \acl{FST} framework and its implementation OpenFST  
determine the shape of the Kaldi data structures.
Kaldi uses \ac{FST} as underlaying representation for \ac{LM}, partially for \ac{AM}, lexicon and 
also for representing transformation between text, pronunciation and triphones.

The \ac{FST} framework provides well studied graph operations\cite{mohri2002weighted},
which can be effectively use for acoustic modeling.
Using the \ac{FST} framework the speech decoding task is expressed as
a beam search in a graph, which is well studied problem.

The OpenFST library implements memory efficient representation of \ac{FST} and
provides standardized efficient operations.
As stated in \cite{mohri2002weighted} and \cite{povey2011kaldi} the operations can be effectively used
for speech decoding. 

The \ac{FST} graphs used for \ac{AM} model training and speech decoding
can be constructed as sequence of standardized OpenFST operations.\cite{mohri2002weighted}.
Decoding is performed using a final result of training, so called {\it decoding graph} $HCLG$. 
\begin{equation} \label{eq:hclg}
HCLG = H\circ C\circ L\circ G
\end{equation}.

The symbol $\circ$ represents an associative binary operation of composition on \acp{FST}.
Namely, the transducers appearing in~Equation~\ref{eq:hclg} are:
\begin{enumerate}
    % source  http://kaldi.sourceforge.net/graph.html
    \item G is an acceptor that encodes the grammar or language model.
    \item L is the lexicon. Its input symbols are phones. Its output symbols are words.
    \item C represents the relationship between context-dependent phones on input and phones on output.
    \item H contains the \ac{HMM} definitions, that take as input id number of~\acp{PDF} and return context-dependent phones.
\end{enumerate}

Following one liner illustrates how Kaldi creates the decoding 
graph\footnote{Kaldi tutorial on building $HCLG$: \url{http://kaldi.sourceforge.net/graph_recipe_test.html}}.\cite{mohri2002weighted}
\begin{equation}
   HCLG = asl(min(rds(det(H' o min(det(C o min(det(L o G)))))))) 
\end{equation}

\todo{define semiring}


% \subsubsection*{OpenFST datastructures in Kaldi}
% We would like to introduce toy examples of construction such data structures which are used in Kaldi.
% Note that the~proper introduction to \ac{FST} framework for speech recognition we recommend\cite{mohri2002weighted}.
% The Kaldi decoding graph construction is very well described in \href{http://kaldi.sourceforge.net/graph\_recipe\_test.html}{Kaldi documentation}.
% We feel that a few figures of toy speech recognition problem will help us explain algorithms,
% which we describe, in Chapters~\ref{cha:train} and~\ref{cha:decoder}.
% The inspiration for such attitude was \href{http://vpanayotov.blogspot.cz/2012/06/kaldi-decoding-graph-construction.html}{Vassil Panayotov post}.
% 
% \todo{Introduction from:
%  OPENFST explanation in Finite State Transducers mechanism in speech recognition -Dan Povey
%  OPENFST article: OpenFst: A General and Efficient Weighted Finite-State Transducer Library Cyril Allauzen1 , Michael Riley2,3 , Johan Schalkwyk2 , Wojciech Skut2 , and Mehryar Mohri1
%  SPEECH RECOGNITION WITH WEIGHTED FINITE-STATE TRANSDUCERS Mehryar Mohri}
% 
% 
% \subsubsection*{\todo{Sources - cite them!}} % (fold)
% 
% \begin{itemize}
%     \item \todo{Consult Vassil blogpost}
%     \item \href{http://kaldi.sourceforge.net/graph.html} {Decoding graph construction in Kaldi}
%     \item \href{http://kaldi.sourceforge.net/lattices.html} {Lattices in Kaldi}
% \end{itemize}
% 
% \subsection{Decoding graph construction in~Kaldi} % (fold)
% % FIXME add source http://kaldi.sourceforge.net/graph.html
% % FIXME consult Vassil blogpost
% 
% Let us explain the shortcuts in the list below. Note that the operation are described in detail
% at page \href{http://kaldi.sourceforge.net/fst_algo.html#fst_algo_stochastic} {Finite State Transducer algorithms in Kaldi}. 
% % The source code of these operations is in fstext and corresponding command-line program are in fstbin/
% \begin{itemize}
%     \item asl - Add self loops to \ac{FST}
%     \item rds - Remove disambiguation symbols from \ac{FST}
%     \item H' is \ac{FST} H without self loops
%     \item min -\ac{FST} minimization
%     \item $A\circ B$  - Composition of \ac{FST} $A$ and $B$.
%     \item det - Determinization of \ac{FST}
% \end{itemize}
% 
% {\bf Kaldi stochasticity} - weights of outgoing arcs sum to 1.
% 
% 
% \subsubsection*{Kaldi decoders} % (fold)
% \begin{itemize}
%     \item SimpleDecoder(Beam width) - straightforward implementation of Viterbi algorithm
%     \item LatticeSimpleDecoder(Beam width d, Lattice delta $\delta$), where $ \delta \le d$
% 
% \end{itemize}
% 
% \subsection*{Decoding algorithm}
% \label{sub:dec_algorithm}
% The Viterbi algorithm is the~basic search algorithm for \ac{HMM}. 
% Briefly
% However
% 
% \subsubsection{Decoding with lattices}
% \label{sub:lattice}
% \todo{what are lattices, why is they are good, depth of lattices for dialog system,
% problem of backward search for lattices}
% In~Subsection~\ref{sub:lattice} we will describe lattices as another output format convenient for dialog systems.
